{"cells":[{"cell_type":"markdown","metadata":{"cell_id":1,"id":"nHMQmdPUR7TJ"},"source":["\n","\n","## Introduction\n","This competition is a user behavior prediction challenge in the news recommendation scenario. The project is based on news recommendation in a news app, with the goal of requiring us to predict the user's future click behavior based on their historical browsing and clicking behavior data, specifically the last news article the user clicked on. The design of this project is intended to introduce participants to some business context in recommender systems and address practical problems.\n","\n","\n","## Data Overview\n","The data comes from user interaction data on a news app platform, including 300,000 users, nearly 3 million clicks, and over 360,000 different news articles. Each news article is represented by an embedding vector. To ensure fairness in the competition, data from 200,000 users is used as the training set, data from 50,000 users is used as test set A, and data from another 50,000 users is used as test set B. For specific data tables and parameters, please refer to the project documentation. Let's discuss how to understand and effectively proceed with the next steps with such data.<br><br>\n","## Evaluation \n","Understanding the evaluation method is important. To understand the evaluation method, we need to look at the final submission file in conjunction with the sample.submit.csv. According to the sample.submit.csv, the format of our final submission is to provide five recommended articles for each user, sorted from front to back based on the click probability. However, for each user, there is only one true answer, which is the article they actually clicked on for their last interaction. So, we need to check whether one of the five recommended articles matches the user's actual last click.\n","for example, for user1, we will submit:\n",">user1, article1, article2, article3, article4, article5.\n","\n","the evaluation formula will be：\n","$$\n","score(user) = \\sum_{k=1}^5 \\frac{s(user, k)}{k}\n","$$\n","\n","\n","If article1 is the article that the user actually clicked on, which means article1 is a hit, then s(user1,1) = 1, and s(user1,2-4) are all 0. If article2 is the article the user clicked on, then s(user1,2) = 1/2, and s(user1,1,3,4,5) are all 0. In other words, score(user) is the reciprocal of the position where the hit occurred. If there is no hit, then score(user1) = 0. This is a reasonable evaluation method because it rewards hitting the correct answer by giving it a higher score, and the score is inversely proportional to the position of the hit.\n","\n","## Challenge\n","Based on the project introduction, it is essential to first understand the goal of this competition: predict the last news article a user will click on based on their historical browsing and clicking behavior data. When looking at this goal, you'll notice that this competition is different from conventional structured competitions in two main aspects:\n","\n","- Objective: The objective is to predict the last news article a user will click on. In other words, the goal is to recommend news articles to users, unlike previous problems where the goal was to predict a single value or a data category.\n","\n","- Data: The data provided is not the usual feature-label data. Instead, it is based on real business scenarios, containing user click logs.\n","\n","With this project, our approach should be to align it with the goal and transform the prediction problem into a supervised learning problem (features + labels) to enable modeling with ML and DL techniques. Naturally, you might have several questions in your mind, such as: How can we transform it into a supervised learning problem? What kind of supervised learning problem should it be? What features are available for us to use? What models can be tried? Given the task of recommending articles to tens of thousands of users, what strategies can we employ?\n","\n","Of course, these questions won't have immediate answers as soon as you see the project, but once you have these questions, you can start finding solutions. For example, regarding the second question - how to transform it into a supervised learning problem: Since the goal is to predict the last article a user will click on from 360,000 articles, your first thought might be that this could be a multi-class classification problem (selecting one out of 360,000 classes). However, handling such a massive classification problem might be challenging. So, can it be transformed differently? If we can predict the probability that a user will click on a specific article as their last click, wouldn't that indirectly solve the problem? The article with the highest probability would be the news article the user is likely to click on last. This way, the original problem becomes a click-through rate prediction problem (user, article) -> probability of a click (soft classification), which is a classification problem in supervised learning, and this guides your choice of models, such as a straightforward logistic regression model.\n","\n","Now, with this understanding, the solution to the project should have a clear direction. It's crucial to first transform it into a classification problem, where the label is whether a user will click on a particular article. The features for the classification problem would include user and article information, and you need to train a classification model to predict the probability of a user clicking on a specific article for their last interaction. This leads to more questions: how to convert it into a supervised learning problem, how to create training and testing datasets, what features to use, what models to try, and how to deal with the large number of articles and users. There's also the question of how to make final predictions. These questions will guide your approach to tackling the project. "]},{"cell_type":"markdown","metadata":{"cell_id":2,"id":"zHDK-jXqR7TL"},"source":["# Baseline"]},{"cell_type":"markdown","metadata":{"cell_id":3,"id":"P7J5n4mvR7TM"},"source":["## 导包"]},{"cell_type":"code","execution_count":7,"metadata":{"ExecuteTime":{"end_time":"2020-11-16T07:46:49.678700Z","start_time":"2020-11-16T07:46:49.673336Z"},"cell_id":4,"executionInfo":{"elapsed":505,"status":"ok","timestamp":1694918945685,"user":{"displayName":"Whale G","userId":"00337772334632878767"},"user_tz":-480},"id":"6yGQydE0R7TM"},"outputs":[],"source":["# Import packages\n","import time  # Import the time module for handling time-related operations.\n","import math  # Import the math module, which provides mathematical functions and constants.\n","import os  # Import the os module for interacting with the operating system.\n","from tqdm import tqdm  # Import the tqdm module for creating progress bars to visualize the progress of iterations.\n","import gc  # Import the garbage collection module for releasing memory space.\n","import pickle  # Import the pickle module for serializing and deserializing Python objects.\n","import random  # Import the random module for generating random numbers.\n","from datetime import datetime  # Import the datetime module for handling date and time.\n","from operator import itemgetter  # Import the itemgetter function from the operator module for retrieving elements based on an index or key.\n","import numpy as np  # Import the NumPy library, which provides high-performance numerical computing capabilities.\n","import pandas as pd  # Import the Pandas library, which offers data analysis and manipulation functionalities.\n","import warnings  # Import the warnings module for controlling the display of warning messages.\n","from collections import defaultdict  # Import the defaultdict class from the collections module, which provides a dictionary that allows setting default values.\n","import collections  # Import the collections module, which provides commonly used collection classes.\n","warnings.filterwarnings('ignore')  # Ignore the display of warning messages."]},{"cell_type":"code","execution_count":8,"metadata":{"ExecuteTime":{"end_time":"2020-11-16T07:48:34.240098Z","start_time":"2020-11-16T07:48:34.236370Z"},"cell_id":5,"executionInfo":{"elapsed":481,"status":"ok","timestamp":1694918984855,"user":{"displayName":"Whale G","userId":"00337772334632878767"},"user_tz":-480},"id":"7S_25dg6R7TN"},"outputs":[],"source":["data_path = '../data/data_raw/' # save raw data\n","save_path = '../data/temp_results/' # save temperary result"]},{"cell_type":"markdown","metadata":{"cell_id":6,"id":"XFtKDcsiR7TN"},"source":["## df Save storage by using sample data "]},{"cell_type":"code","execution_count":9,"metadata":{"cell_id":7,"executionInfo":{"elapsed":476,"status":"ok","timestamp":1694919041616,"user":{"displayName":"Whale G","userId":"00337772334632878767"},"user_tz":-480},"id":"kQegksAcR7TN"},"outputs":[],"source":["# A standard function for memory optimization\n","def reduce_mem(df):\n","    starttime = time.time()  # Record the start time of the function\n","    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']  # List of numeric data types\n","    start_mem = df.memory_usage().sum() / 1024**2  # Calculate the memory usage of the DataFrame (in Mb)\n","\n","    # Iterate through each column of the DataFrame\n","    for col in df.columns:\n","        col_type = df[col].dtypes  # Get the data type of the column\n","        if col_type in numerics:  # If the column's data type is numeric\n","            c_min = df[col].min()  # Get the minimum value in the column\n","            c_max = df[col].max()  # Get the maximum value in the column\n","\n","            # Check if there are missing values in the minimum and maximum values\n","            if pd.isnull(c_min) or pd.isnull(c_max):\n","                continue\n","\n","            # Choose the appropriate data type conversion based on the data type's range\n","            if str(col_type)[:3] == 'int':\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)\n","            else:\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float16)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)\n","\n","    end_mem = df.memory_usage().sum() / 1024**2  # Calculate the memory usage of the DataFrame after conversion (in Mb)\n","    print('-- Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction), time spend:{:2.2f} min'.format(end_mem,\n","                                                                                                  100*(start_mem-end_mem)/start_mem,\n","                                                                                                  (time.time()-starttime)/60))\n","    return df"]},{"cell_type":"markdown","metadata":{"cell_id":8,"id":"_F3Dp1qSR7TO"},"source":["## 读取采样或全量数据"]},{"cell_type":"markdown","metadata":{"cell_id":32,"id":"tD7yjrAiR7TO"},"source":["在代码开发和调试过程中，有时候我们需要处理大规模的数据集，但是由于数据量太大或计算资源有限，无法对整个数据集进行完整的操作和调试。这时候就可以采用\"debug模式\"来从训练集中划出一部分数据作为样本来进行代码的调试和验证。"]},{"cell_type":"code","execution_count":10,"metadata":{"ExecuteTime":{"end_time":"2020-11-16T07:48:50.619963Z","start_time":"2020-11-16T07:48:50.611667Z"},"cell_id":9,"executionInfo":{"elapsed":12,"status":"ok","timestamp":1694919505161,"user":{"displayName":"Whale G","userId":"00337772334632878767"},"user_tz":-480},"id":"1t4cvjoJR7TO"},"outputs":[],"source":["# Debug mode: Sample a portion of data from the training set for code debugging\n","def get_all_click_sample(data_path, sample_nums=10000):\n","    \"\"\"\n","    从训练集中采样一部分数据用于调试\n","    data_path: 原数据的存储路径\n","    sample_nums: 采样数目（由于机器内存限制，可以采样少量用户）\n","    \"\"\"\n","    all_click = pd.read_csv(data_path + 'train_click_log.csv')  \n","    all_user_ids = all_click.user_id.unique()  # Get unique identifiers for all users\n","\n","   # Randomly select a specified number of users from all users as sampled users\n","    sample_user_ids = np.random.choice(all_user_ids, size=sample_nums, replace=False)\n","    all_click = all_click[all_click['user_id'].isin(sample_user_ids)]  # Retain click data for sampled users\n","\n","    all_click = all_click.drop_duplicates((['user_id', 'click_article_id', 'click_timestamp']))  # 去除重复的点击数据\n","    return all_click\n","\n","# Read click data, which is divided into online and offline. If it is for online submission results, the test set's click data should be merged into the overall data.\n","# If it is for offline validation of the model's effectiveness or feature effectiveness, you can use only the training set.\n","def get_all_click_df(data_path='./data_raw/', offline=True):\n","    if offline:\n","        all_click = pd.read_csv(data_path + 'train_click_log.csv')  # Read the click data from the training set\n","    else:\n","        trn_click = pd.read_csv(data_path + 'train_click_log.csv') # Read the click data from the training set\n","        tst_click = pd.read_csv(data_path + 'testA_click_log.csv')   # Read the click data from the test set\n","\n","        all_click = trn_click.append(tst_click)  # Combine the click data from the training set and test set\n","\n","    all_click = all_click.drop_duplicates((['user_id', 'click_article_id', 'click_timestamp']))  # Remove duplicate click data\n","    return all_click\n"]},{"cell_type":"code","execution_count":11,"metadata":{"cell_id":10,"executionInfo":{"elapsed":4302,"status":"ok","timestamp":1694919511177,"user":{"displayName":"Whale G","userId":"00337772334632878767"},"user_tz":-480},"id":"03EDRcyDR7TO"},"outputs":[],"source":["# 全量训训练集\n","all_click_df = get_all_click_df(data_path, offline=False)"]},{"cell_type":"code","execution_count":12,"metadata":{"cell_id":30,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1694919514732,"user":{"displayName":"Whale G","userId":"00337772334632878767"},"user_tz":-480},"id":"E1vZouplR7TP","outputId":"697090c6-0f1a-4e04-e039-0b20878c9557"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 1630633 entries, 0 to 518009\n","Data columns (total 9 columns):\n"," #   Column               Non-Null Count    Dtype\n","---  ------               --------------    -----\n"," 0   user_id              1630633 non-null  int64\n"," 1   click_article_id     1630633 non-null  int64\n"," 2   click_timestamp      1630633 non-null  int64\n"," 3   click_environment    1630633 non-null  int64\n"," 4   click_deviceGroup    1630633 non-null  int64\n"," 5   click_os             1630633 non-null  int64\n"," 6   click_country        1630633 non-null  int64\n"," 7   click_region         1630633 non-null  int64\n"," 8   click_referrer_type  1630633 non-null  int64\n","dtypes: int64(9)\n","memory usage: 124.4 MB\n"]}],"source":["all_click_df.info()"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":472,"status":"ok","timestamp":1690686284326,"user":{"displayName":"Whale G","userId":"00337772334632878767"},"user_tz":-480},"id":"kRu5NWniwTv9","outputId":"bb70ad5b-19f8-4468-c89b-4bee8566aa3a"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>click_article_id</th>\n","      <th>click_timestamp</th>\n","      <th>click_environment</th>\n","      <th>click_deviceGroup</th>\n","      <th>click_os</th>\n","      <th>click_country</th>\n","      <th>click_region</th>\n","      <th>click_referrer_type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>199999</td>\n","      <td>160417</td>\n","      <td>1507029570190</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>17</td>\n","      <td>1</td>\n","      <td>13</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>199999</td>\n","      <td>5408</td>\n","      <td>1507029571478</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>17</td>\n","      <td>1</td>\n","      <td>13</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>199999</td>\n","      <td>50823</td>\n","      <td>1507029601478</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>17</td>\n","      <td>1</td>\n","      <td>13</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>199998</td>\n","      <td>157770</td>\n","      <td>1507029532200</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>17</td>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>199998</td>\n","      <td>96613</td>\n","      <td>1507029671831</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>17</td>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   user_id  click_article_id  click_timestamp  click_environment  \\\n","0   199999            160417    1507029570190                  4   \n","1   199999              5408    1507029571478                  4   \n","2   199999             50823    1507029601478                  4   \n","3   199998            157770    1507029532200                  4   \n","4   199998             96613    1507029671831                  4   \n","\n","   click_deviceGroup  click_os  click_country  click_region  \\\n","0                  1        17              1            13   \n","1                  1        17              1            13   \n","2                  1        17              1            13   \n","3                  1        17              1            25   \n","4                  1        17              1            25   \n","\n","   click_referrer_type  \n","0                    1  \n","1                    1  \n","2                    1  \n","3                    5  \n","4                    5  "]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["all_click_df.head()"]},{"cell_type":"markdown","metadata":{"cell_id":11,"id":"b212AQdlR7TP"},"source":["## create a dictionary that maps users to articles and their corresponding click times"]},{"cell_type":"code","execution_count":14,"metadata":{"ExecuteTime":{"end_time":"2020-11-16T07:56:39.800240Z","start_time":"2020-11-16T07:56:39.793541Z"},"cell_id":12,"executionInfo":{"elapsed":638,"status":"ok","timestamp":1694920240997,"user":{"displayName":"Whale G","userId":"00337772334632878767"},"user_tz":-480},"id":"7an1g2cRR7TP"},"outputs":[],"source":["# Get the user's article-click time sequence based on click time: {user1: [(item1, time1), (item2, time2), ...]...}\n","def get_user_item_time(click_df):\n","    # Sort the click DataFrame by click timestamp\n","    click_df = click_df.sort_values('click_timestamp')\n","\n","    def make_item_time_pair(df):\n","        # Create a list of article IDs and corresponding timestamps\n","        return list(zip(df['click_article_id'], df['click_timestamp']))\n","\n","    # Group by user, generate a list of article ID and timestamp pairs for each user, reset the index, and rename the columns\n","    user_item_time_df = click_df.groupby('user_id')[['click_article_id', 'click_timestamp']].apply(lambda x: make_item_time_pair(x))\\\n","                                                            .reset_index().rename(columns={0: 'item_time_list'})\n","    # Create a dictionary with user IDs as keys and lists of article ID and timestamp pairs as values\n","    user_item_time_dict = dict(zip(user_item_time_df['user_id'], user_item_time_df['item_time_list']))\n","\n","    return user_item_time_dict\n"]},{"cell_type":"markdown","metadata":{"cell_id":13,"id":"Jeqv349SR7TQ"},"source":["## catch the top k clicked article"]},{"cell_type":"code","execution_count":15,"metadata":{"cell_id":14,"executionInfo":{"elapsed":533,"status":"ok","timestamp":1694920242147,"user":{"displayName":"Whale G","userId":"00337772334632878767"},"user_tz":-480},"id":"6nFVXycvR7TQ"},"outputs":[],"source":["# Get the most recently clicked articles\n","def get_item_topk_click(click_df, k):\n","    \"\"\"\n","    Get the top k article IDs with the most clicks.\n","\n","    Parameters:\n","        click_df: DataFrame containing click data.\n","        k: Number of top article IDs to retrieve.\n","\n","    Returns:\n","        topk_click: List of the top k article IDs with the most clicks.\n","    \"\"\"\n","    # Use the value_counts() function to count the occurrences of each article ID in the click_df DataFrame's click_article_id column.\n","    # Then, use index slicing [:k] to select the top k articles with the highest click counts.\n","    topk_click = click_df['click_article_id'].value_counts().index[:k]\n","    return topk_click"]},{"cell_type":"markdown","metadata":{"cell_id":15,"id":"51iIzmT4R7TQ"},"source":["## itemcf Calculation of the similarity matrix"]},{"cell_type":"code","execution_count":16,"metadata":{"ExecuteTime":{"end_time":"2020-11-16T07:51:07.577037Z","start_time":"2020-11-16T07:51:07.568098Z"},"cell_id":16,"executionInfo":{"elapsed":3,"status":"ok","timestamp":1694920244494,"user":{"displayName":"Whale G","userId":"00337772334632878767"},"user_tz":-480},"id":"390FVt7XR7TQ"},"outputs":[],"source":["def itemcf_sim(df):\n","    \"\"\"\n","    Calculation of the similarity matrix between articles\n","\n","    Parameters:\n","        df: Data table\n","        item_created_time_dict: Dictionary of article creation times\n","\n","    Returns:\n","        i2i_sim_: Matrix of similarity between articles\n","\n","    Idea:\n","    Collaborative filtering based on items (for details, refer to the previous team learning on basic recommendation systems).\n","    In the multi-recall section, a recall strategy based on association rules will be added.\n","    \"\"\"\n","\n","    # Call a function to obtain a dictionary of user-item-click time data\n","    user_item_time_dict = get_user_item_time(df)\n","\n","    # Calculate item similarity\n","    i2i_sim = {}  # Store a dictionary of item-item similarity\n","    item_cnt = defaultdict(int)  # Dictionary to count item occurrences\n","\n","    # Iterate through the user-item-click time data dictionary\n","    for user, item_time_list in tqdm(user_item_time_dict.items()):\n","        # Consider time factors when optimizing item-based collaborative filtering\n","\n","        # Iterate through the list of items and their click times for the same user\n","        for i, i_click_time in item_time_list:\n","            # Count item occurrences\n","            item_cnt[i] += 1\n","            i2i_sim.setdefault(i, {})\n","\n","            # Iterate through the list of other items and their click times for the same user\n","            for j, j_click_time in item_time_list:\n","                if i == j:\n","                    continue\n","                i2i_sim[i].setdefault(j, 0)\n","                i2i_sim[i][j] += 1 / math.log(len(item_time_list) + 1)\n","\n","    i2i_sim_ = i2i_sim.copy()\n","\n","    # Further process and optimize the similarity dictionary\n","    for i, related_items in i2i_sim.items():\n","        for j, wij in related_items.items():\n","            i2i_sim_[i][j] = wij / math.sqrt(item_cnt[i] * item_cnt[j])\n","\n","    # Save the obtained similarity matrix locally\n","    pickle.dump(i2i_sim_, open(save_path + 'itemcf_i2i_sim.pkl', 'wb'))\n","\n","    return i2i_sim_"]},{"cell_type":"code","execution_count":17,"metadata":{"ExecuteTime":{"end_time":"2020-11-16T07:53:10.038470Z","start_time":"2020-11-16T07:51:11.281176Z"},"cell_id":17,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":77652,"status":"ok","timestamp":1694920323323,"user":{"displayName":"Whale G","userId":"00337772334632878767"},"user_tz":-480},"id":"TUCo1UrnR7TQ","outputId":"981a92b1-9b8d-4578-ea24-29187d595254"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 250000/250000 [00:17<00:00, 13928.38it/s]\n"]}],"source":["i2i_sim = itemcf_sim(all_click_df)"]},{"cell_type":"markdown","metadata":{"cell_id":18,"id":"vVCIQ84uR7TQ"},"source":["## itemcf article recommendation "]},{"cell_type":"code","execution_count":18,"metadata":{"ExecuteTime":{"end_time":"2020-11-16T08:03:18.383215Z","start_time":"2020-11-16T08:03:18.373432Z"},"cell_id":19,"executionInfo":{"elapsed":531,"status":"ok","timestamp":1694920696585,"user":{"displayName":"Whale G","userId":"00337772334632878767"},"user_tz":-480},"id":"1a1ITx-XR7TQ"},"outputs":[],"source":["def item_based_recommend(user_id, user_item_time_dict, i2i_sim, sim_item_topk, recall_item_num, item_topk_click):\n","    \"\"\"\n","    Recommendation based on item-based collaborative filtering.\n","    \n","    Parameters:\n","        user_id: User ID\n","        user_item_time_dict: Dictionary of user-clicked article sequences based on click time {user1: [(item1, time1), (item2, time2), ...]...}\n","        i2i_sim: Dictionary, article similarity matrix\n","        sim_item_topk: Integer, choose the top k most similar articles to the current article\n","        recall_item_num: Integer, the number of recalled articles in the end\n","        item_topk_click: List, the list of most-clicked articles for user recall completion\n","    \n","    Returns:\n","        item_rank: Recommended articles {item1: score1, item2: score2, ...}\n","        \n","    Note: In the multi-recall part, a recall strategy based on association rules will be added.\n","    \"\"\"\n","\n","    # Get the articles that the user has interacted with in the past\n","    user_hist_items = user_item_time_dict[user_id]  # Get the list of articles the user has clicked on\n","    user_hist_items_ = {user_id for user_id, _ in user_hist_items}  # Convert the list of articles the user has clicked on to a set for easy lookup\n","\n","    item_rank = {}  # Store a dictionary of articles and their similarity scores\n","    for loc, (i, click_time) in enumerate(user_hist_items):\n","        # Iterate through the list of articles the user has clicked on and their corresponding click times\n","        for j, wij in sorted(i2i_sim[i].items(), key=lambda x: x[1], reverse=True)[:sim_item_topk]:\n","            # Iterate through the top sim_item_topk articles most similar to the current article and their similarity scores\n","            if j in user_hist_items_:\n","                continue  # If the similar article is already in the user's historical click list, skip it\n","            item_rank.setdefault(j, 0)\n","            item_rank[j] += wij  # Accumulate the similarity score of the similar articles\n","\n","    # If there are less than recall_item_num articles, complete with popular items\n","    if len(item_rank) < recall_item_num:\n","        for i, item in enumerate(item_topk_click):\n","            if item in item_rank.items():  # If the completed article is already in the previous list, skip it\n","                continue\n","            item_rank[item] = -i - 100  # Assign a negative score to the completed article (arbitrarily set)\n","            if len(item_rank) == recall_item_num:\n","                break  # Exit the loop after reaching the specified number of recalled articles\n","\n","    item_rank = sorted(item_rank.items(), key=lambda x: x[1], reverse=True)[:recall_item_num]  # Sort articles in descending order based on score and truncate to the specified number of articles\n","\n","    return item_rank  # Return the list of recalled articles, including articles and their scores"]},{"cell_type":"markdown","metadata":{"cell_id":20,"id":"Dnq-N9-uR7TR"},"source":["## To recommend articles for each user based on item-based collaborative filtering"]},{"cell_type":"code","execution_count":19,"metadata":{"ExecuteTime":{"end_time":"2020-11-16T10:15:01.109798Z","start_time":"2020-11-16T08:11:07.233787Z"},"cell_id":21,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3037896,"status":"ok","timestamp":1694923736171,"user":{"displayName":"Whale G","userId":"00337772334632878767"},"user_tz":-480},"id":"vKbFBCl-R7TR","outputId":"22396863-630b-4e2d-cd54-a866808b1109","scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 250000/250000 [22:17<00:00, 186.95it/s] \n"]}],"source":["# Define\n","user_recall_items_dict = collections.defaultdict(dict)\n","\n","# Get the user-item-click time dictionary\n","user_item_time_dict = get_user_item_time(all_click_df)\n","\n","# Load item-item similarity\n","i2i_sim = pickle.load(open(save_path + 'itemcf_i2i_sim.pkl', 'rb'))\n","\n","# Number of similar articles to consider\n","sim_item_topk = 10\n","\n","# Number of recalled articles\n","recall_item_num = 10\n","\n","# User recall completion with popular items\n","item_topk_click = get_item_topk_click(all_click_df, k=50)\n","\n","# Loop through all users\n","for user in tqdm(all_click_df['user_id'].unique()):\n","    # Recommend articles for each user based on item-based collaborative filtering\n","    user_recall_items_dict[user] = item_based_recommend(user, user_item_time_dict, i2i_sim,\n","                                                        sim_item_topk, recall_item_num, item_topk_click)"]},{"cell_type":"markdown","metadata":{"cell_id":22,"id":"CzKJYozER7TR"},"source":["## transform recall dictionary to df"]},{"cell_type":"code","execution_count":20,"metadata":{"ExecuteTime":{"end_time":"2020-11-16T10:16:36.647466Z","start_time":"2020-11-16T10:16:24.791219Z"},"cell_id":23,"id":"rnklCiaPR7TR","outputId":"96d89116-76c3-4f40-da17-01350a3cf9d4","scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 250000/250000 [00:04<00:00, 53961.20it/s]\n"]}],"source":["\n","user_item_score_list = []\n","\n","for user, items in tqdm(user_recall_items_dict.items()):\n","    for item, score in items:\n","        user_item_score_list.append([user, item, score])\n","\n","recall_df = pd.DataFrame(user_item_score_list, columns=['user_id', 'click_article_id', 'pred_score'])"]},{"cell_type":"markdown","metadata":{"cell_id":24,"id":"JgEXMayWR7TR"},"source":["## submit file"]},{"cell_type":"code","execution_count":21,"metadata":{"ExecuteTime":{"end_time":"2020-11-16T10:16:46.268341Z","start_time":"2020-11-16T10:16:46.259293Z"},"cell_id":25,"id":"vKukhSKYR7TR"},"outputs":[],"source":["# create submit file\n","def submit(recall_df, topk=5, model_name=None):\n","    recall_df = recall_df.sort_values(by=['user_id', 'pred_score'])\n","    recall_df['rank'] = recall_df.groupby(['user_id'])['pred_score'].rank(ascending=False, method='first')\n","\n","    # detect wether every user has 5 articles\n","    tmp = recall_df.groupby('user_id').apply(lambda x: x['rank'].max())\n","    assert tmp.min() >= topk\n","\n","    del recall_df['pred_score']\n","    submit = recall_df[recall_df['rank'] <= topk].set_index(['user_id', 'rank']).unstack(-1).reset_index()\n","\n","    submit.columns = [int(col) if isinstance(col, int) else col for col in submit.columns.droplevel(0)]\n","    # define column name\n","    submit = submit.rename(columns={'': 'user_id', 1: 'article_1', 2: 'article_2',\n","                                                  3: 'article_3', 4: 'article_4', 5: 'article_5'})\n","\n","    save_name = save_path + model_name + '_' + datetime.today().strftime('%m-%d') + '.csv'\n","    submit.to_csv(save_name, index=False, header=True)"]},{"cell_type":"code","execution_count":22,"metadata":{"ExecuteTime":{"end_time":"2020-11-16T10:17:42.254328Z","start_time":"2020-11-16T10:17:32.211862Z"},"cell_id":26,"id":"LA9NlpuHR7TS"},"outputs":[],"source":["# create test set\n","tst_click = pd.read_csv(data_path + 'testA_click_log.csv')\n","tst_users = tst_click['user_id'].unique()\n","\n","# select the users from the recall data that are in the test set, you can follow these steps\n","tst_recall = recall_df[recall_df['user_id'].isin(tst_users)]\n","\n","# submit final file, which is the baseline\n","submit(tst_recall, topk=5, model_name='itemcf_baseline')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"max_cell_id":34,"tianchi_metadata":{"competitions":[],"datasets":[],"description":"","notebookId":"130006","source":"dsw"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"170px"},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":0}
